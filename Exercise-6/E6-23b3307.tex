\documentclass{article}
\usepackage{nirav-ds203}

\newcommand{\myname}{Nirav Bhattad}
\newcommand{\topicname}{DS203 : Exercise 6}

\begin{document}

\thispagestyle{empty}

\titleBC

\input{tex/P1}
\input{tex/P2}
\input{tex/P3}

\section{Introduction}
This report summarizes the learnings from three Python scripts that involve data analysis, data cleaning, and machine learning techniques. The scripts demonstrate various methodologies for exploring, processing, and visualizing data to extract meaningful insights.

\subsection{Problem 1: Analyzing HT R Phase Current Data}
The first script focuses on analyzing a dataset containing the HT R Phase Current over time. The key tasks and learnings from this analysis are as follows:

\subsubsection{Exploratory Data Analysis (EDA)}
The initial step involved loading the data and performing descriptive statistics to understand the dataset better. The \texttt{describe()} method provided an overview of the data distribution, highlighting key metrics such as mean, median, and standard deviation.

\subsubsection{Visualization of Current over Time}
Using \texttt{Plotly}, the script visualized the HT R Phase Current over time, allowing for a dynamic representation of trends and anomalies. This visualization is crucial for identifying periods of instability.

\subsubsection{Identifying Unstable Periods}
A specific two-week period (July 30, 2019, to August 14, 2019) was identified as unstable. The script plotted the current values during this period to visualize fluctuations.

\subsubsection{Outlier Handling and Data Imputation}
Various methods were applied to handle outliers and missing data:
\begin{itemize}
    \item \textbf{Imputation:} Replacing outliers with the mean or median values.
    \item \textbf{Trimming:} Removing outliers based on the 10th and 90th percentiles.
    \item \textbf{Capping:} Setting maximum and minimum thresholds for current values.
    \item \textbf{Robust Estimation:} Using RANSAC regression to estimate current values while ignoring outliers.
    \item \textbf{Loess Smoothing:} Applying local regression to identify trends in the data.
\end{itemize}

\subsubsection{Visualization of Processed Data}
The script generated several plots to visualize the original and processed data, demonstrating the impact of different outlier handling methods. This included comparisons of the original data with mean and median imputed data, trimmed data, capped data, robust estimations, and Loess smoothed data.

\subsection{Problem 2: Data Cleaning and Feature Selection}
The second script focuses on data cleaning and feature selection techniques applied to a dataset with 100 columns. The key tasks and learnings from this analysis are as follows:

\subsubsection{Data Loading and Initial Exploration}
Data was loaded from a CSV file, followed by initial explorations such as checking for null values and obtaining summary statistics.

\subsubsection{Handling Low Variance Columns}
Columns with low variance (less than 0.05) were identified and removed, as they provide little useful information for predictive modeling.

\subsubsection{Missing Value Imputation}
Missing values in numeric columns were replaced with the mean of those columns, ensuring that the dataset remains usable for further analysis.

\subsubsection{Outlier Detection and Removal}
Outliers were detected using the Interquartile Range (IQR) method. Rows containing outliers were removed to improve the quality of the dataset.

\subsubsection{Normalization and Standardization}
The numeric data was standardized using \texttt{StandardScaler} from \texttt{sklearn}, allowing for the normalization of features across different scales, which is essential for machine learning algorithms.

\subsubsection{Correlation Analysis}
A correlation heatmap was generated to visualize relationships between features. Highly correlated features (with a threshold of 0.8) were identified, and one feature from each pair was dropped to reduce multicollinearity.

\subsection{Problem 3: Dimensionality Reduction with PCA and t-SNE}
The third script focuses on applying dimensionality reduction techniques, specifically PCA (Principal Component Analysis) and t-SNE (t-distributed Stochastic Neighbor Embedding), to the MNIST dataset and another dataset named E6. The key tasks and learnings from this analysis are as follows:

\subsubsection{PCA Analysis on MNIST}
The script begins by loading the MNIST test dataset and separating features from labels. Key steps in the PCA analysis include:
\begin{itemize}
    \item \textbf{Standardization:} The features were standardized using \texttt{StandardScaler} to ensure that each feature contributes equally to the analysis.
    \item \textbf{PCA Application:} PCA was applied to the standardized data, transforming it into a lower-dimensional space. The explained variance ratio and cumulative variance were calculated to determine the effectiveness of the principal components.
    \item \textbf{Elbow Diagram:} An elbow diagram was created to visualize cumulative explained variance by the number of principal components, helping identify an optimal number of components.
    \item \textbf{Scatter Plot:} A scatter plot was generated to visualize the first two principal components, revealing the distribution of the data in the reduced space.
\end{itemize}

\subsubsection{t-SNE Analysis on MNIST}
Following the PCA analysis, t-SNE was applied to further reduce the dataset to two dimensions:
\begin{itemize}
    \item \textbf{t-SNE Application:} The script applied t-SNE to the standardized MNIST data to create a 2D representation, which is particularly effective for visualizing high-dimensional data.
    \item \textbf{t-SNE Scatter Plot:} A scatter plot was generated to visualize the clusters formed by different digits in the MNIST dataset, providing insights into the separability of classes.
\end{itemize}

\subsubsection{t-SNE Analysis on E6 Dataset}
The script also applied t-SNE to another dataset named E6:
\begin{itemize}
    \item \textbf{Data Loading and Cleaning:} The E6 dataset was loaded, and preprocessing steps included replacing erroneous values with NaN, converting timestamps, and handling missing values.
    \item \textbf{Feature Scaling:} Features were scaled using \texttt{StandardScaler} before applying t-SNE.
    \item \textbf{t-SNE Visualization:} A t-SNE scatter plot was created for the E6 dataset, allowing visualization of the distribution of the data in two dimensions.
\end{itemize}

\subsection{Conclusion}
The scripts illustrate essential techniques in data analysis and machine learning, including exploratory data analysis, outlier handling, feature selection, and dimensionality reduction. The application of PCA and t-SNE provides valuable insights into the structure of high-dimensional datasets, facilitating effective visualization and understanding of complex data relationships.

% \subsection{References}
% \begin{itemize}
%     \item \texttt{pandas} documentation: \url{https://pandas.pydata.org/pandas-docs/stable/}
%     \item \texttt{numpy} documentation: \url{https://numpy.org/doc/stable/}
%     \item \texttt{scikit-learn} documentation: \url{https://scikit-learn.org/stable/documentation.html}
%     \item \texttt{matplotlib} documentation: \url{https://matplotlib.org/stable/contents.html}
%     \item \texttt{plotly} documentation: \url{https://plotly.com/python/}
% \end{itemize}

\end{document}