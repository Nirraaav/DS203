\section*{Step 6}

\begin{custombox}[label={box:Q6}]{Step 6}
	Compare the metrics within and across the datasets (train and test) and algorithms. In addition to the variations in metrics, include in your report aspects related to classification boundaries, overfitting, etc.
\end{custombox}

\vspace{10mm}

% Dataset 1 is a good separated dataset, 2 is a bit mixed up and 3 is mixed up. 

\subsection*{Dataset 1}

In the case of Dataset 1, the training and testing data are well separated. The decision boundaries are clear and the model is not overfitting. The accuracy of the model is $100$\% for both training and testing data for all the $10$ classification models. We also observe that the precision, recall and F1 score are very close to $1.0$ for all the $10$ classification models. Classification boundaries are clear and the model is not overfitting on the training data.

\subsection*{Dataset 2}

In the case of Dataset 2, the training and testing data are mixed up. The decision boundaries are not clear and the model is overfitting. The accuracy of the model is $98$\% for the training data and $90$\% on average for the testing data for all the $10$ classification models. We also observe that the precision, recall and F1 score are very close to $1.0$ for all the $10$ classification models. Classification boundaries are not clear and the model is overfitting on the training data.

\subsection*{Dataset 3}

In the case of Dataset 3, the training and testing data are  highly mixed up. The decision boundaries are not clear and the model is overfitting. The accuracy of the model is $90$\% for the training data and $90$\% on average for the testing data for all the $10$ classification models. We also observe that the precision, recall and F1 score are very close to $1.0$ for all the $10$ classification models. Classification boundaries are not clear at all and some of the model is overfitting on the training data.




