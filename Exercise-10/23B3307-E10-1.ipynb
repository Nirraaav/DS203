{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E10-1\n",
    "\n",
    "Nirav Bhattad (23B3307)\n",
    "\n",
    "#### This Notebook illustrates the use of \"MAP-REDUCE\" to calculate price averages using the data contained in nsedata.csv.\n",
    "\n",
    "### <b>Task 1</b>\n",
    "You are required to review the code (refer to the SPARK document where necessary), and <b>add comments / markup explaining the code in each cell</b>. Also explain what each cell is trying to achieve in the overall scheme of things. You may create additional code in each cell to generate debug output that you may need to complete this exercise.\n",
    "### <b>Task 2</b>\n",
    "You are required to write code to solve the problem stated at the end this Notebook\n",
    "### <b>Submission</b>\n",
    "Create and upload a PDF of this Notebook. <b> BEFORE CONVERTING TO PDF and UPLOADING ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS </b>. Short debug outputs of up to 5 lines are acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # import findspark module to locate spark in the system\n",
    "findspark.init() # initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # import pyspark module\n",
    "from pyspark.sql.types import * # import all the functions from pyspark.sql.types module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E10\") # create a spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"nsedata.csv\") # read the csv file into an rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd1.filter(lambda x: \"SYMBOL\" not in x) # here we are using the Lambda function and filter transformation to remove all the elements of the RDD with the word \"SYMBOL\" in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x : x.split(\",\")) # split the rdd by commas using a lambda function and the map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper comment!: The goal is to find out the mean of the OPEN prices and the mean of the CLOSE price in one batch of tasks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_open = rdd2.map(lambda x : (x[0]+\"_open\",float(x[2]))) # here we are using the map function and a lambda function to create a new rdd with the symbol_open and the open price as the key value pair.\n",
    "rdd_close = rdd2.map(lambda x : (x[0]+\"_close\",float(x[5]))) # here we are using the map function and a lambda function to create a new rdd with the symbol_close and the close price as the key value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_united = rdd_open.union(rdd_close) # here we are using the union transformation to combine the two rdds into one rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedByKey = rdd_united.reduceByKey(lambda x,y: x+y) # here we are using the reduceByKey transformation to reduce the rdd by key and sum the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp1 = rdd_united.map(lambda x: (x[0],1)).countByKey() # here we are using the map function and a lambda function to create a new rdd with the symbol and the value 1 as the key value pair. We then use the countByKey function to count the number of times each symbol appears in the rdd.\n",
    "countOfEachSymbol = sc.parallelize(temp1.items()) # here we are using the parallelize function to create an rdd from the dictionary items of the temp1 rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_sum_count = reducedByKey.join(countOfEachSymbol) # here we are using the join transformation to join the reducedByKey rdd and the countOfEachSymbol rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = symbol_sum_count.map(lambda x : (x[0], x[1][0]/x[1][1])) # here we are using the map function and a lambda function to create a new rdd with the symbol and the average price as the key value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted = averages.sortByKey() # here we are using the sortByKey transformation to sort the rdd by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 19:52:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "averagesSorted.saveAsTextFile(\"./averages\") # save the rdd to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() # stop the spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the output files generated in the above step and copy the first 15 lines of any one of the output files into the cell below for reference. Write your comments on the generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SHREYANIND_open', 25.703465346534657)\n",
      "('SHREYAS_close', 102.53706030150751)\n",
      "('SHREYAS_open', 102.4324539363484)\n",
      "('SHRIASTER_close', 8.112228260869564)\n",
      "('SHRIASTER_open', 8.002853260869566)\n",
      "('SHRINATRAJ_close', 75.10040650406505)\n",
      "('SHRINATRAJ_open', 75.13536585365853)\n",
      "('SHRIRAMCIT_close', 1046.2627501012555)\n",
      "('SHRIRAMCIT_open', 1046.2623572296482)\n",
      "('SHRIRAMEPC_close', 81.96172190784152)\n",
      "('SHRIRAMEPC_open', 82.73399353274051)\n",
      "('SHYAMCENT_close', 8.672)\n",
      "('SHYAMCENT_open', 8.718)\n",
      "('SHYAMTEL_close', 32.51556184316896)\n",
      "('SHYAMTEL_open', 32.732659660468876)\n"
     ]
    }
   ],
   "source": [
    "with open(\"./averages/part-00024\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i < 15:\n",
    "            print(line, end='')\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Problem Statement\n",
    "Using the <b>MAP-REDUCE</b> method, write SPARK code that will create the average of HIGH prices for every traded company traded within any 3 continuous months of your choice. Create the appropriate (K,V) pairs so that the averages are simultaneously calculated for each company, as in the above example. Create the output files such that the final data is sorted in <b>descending order</b> of the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E10\") # create a spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.textFile(\"./nsedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=rdd1.filter(lambda x:\"SYMBOL\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=rdd1.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20MICRONS_high_average', 37.75), ('3IINFOTECH_high_average', 45.3), ('3MINDIA_high_average', 3439.95)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_high=rdd2.map(lambda x: (x[0]+\"_high_average\",float(x[3]))) \n",
    "elements = rdd_high.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABBOTINDIA_high_average', 2425757.700000001), ('ABCIL_high_average', 166873.4000000001), ('ACKRUTI_high_average', 87689.35)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reducedByKey_2 = rdd_high.reduceByKey(lambda x,y: x+y)\n",
    "elements = reducedByKey_2.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20MICRONS_high_average', 1237), ('3IINFOTECH_high_average', 1237), ('3MINDIA_high_average', 1237)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp1_2 = rdd_high.map(lambda x: (x[0],1)).countByKey()\n",
    "countOfEachSymbol_2 = sc.parallelize(temp1_2.items())\n",
    "elements = countOfEachSymbol_2.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20MICRONS_high_average', (67564.34999999998, 1237)), ('3IINFOTECH_high_average', (22960.199999999997, 1237)), ('3MINDIA_high_average', (5694089.6499999985, 1237))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "symbol_sum_count_2 = reducedByKey_2.join(countOfEachSymbol_2)\n",
    "temporary_2 = symbol_sum_count_2.sortByKey()\n",
    "elements = temporary_2.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AFTEK_high_average', 8.742150170648463), ('APOLLOTYRE_high_average', 111.61240905416336), ('ASHOKLEY_high_average', 39.14737479806137)]\n"
     ]
    }
   ],
   "source": [
    "averages_2 = symbol_sum_count_2.map(lambda x : (x[0], x[1][0]/x[1][1]))\n",
    "elements = averages_2.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20MICRONS_high_average', 54.61952303961195), ('3IINFOTECH_high_average', 18.561196443007272), ('3MINDIA_high_average', 4603.144421988681)]\n"
     ]
    }
   ],
   "source": [
    "averagesSorted_2 = averages_2.sortByKey()\n",
    "elements = averagesSorted_2.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagesSorted_2.saveAsTextFile(\"./averages-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2=sc.textFile(\"./nsedata.csv\")\n",
    "rdd_2=rdd_2.filter(lambda x:\"SYMBOL\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3MINDIA', 111974.0), ('8KMILES', 12297.65), ('ABBOTINDIA', 57560.149999999994)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp_sample=rdd_2.filter(lambda x:(\"OCT-2014\" or \"NOV-2014\" or \"DEC-2014\") in x)\n",
    "temp=temp_sample.map(lambda x:x.split(\",\"))\n",
    "temp_high=temp.map(lambda x : (x[0],float(x[3])))\n",
    "temp_by_key=temp_high.reduceByKey(lambda x,y : x+y)\n",
    "elements = temp_by_key.take(3)\n",
    "\n",
    "print(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = temp_by_key.map(lambda x : (x[0],1)).countByKey()\n",
    "counts = sc.parallelize(temp_2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "symbol_highsum_count = temp_by_key.join(counts)\n",
    "avg_high = symbol_highsum_count.map(lambda x : (x[0] , x[1][0]/x[1][1]))\n",
    "avgs_desc = avg_high.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ZYLOG', 140.15)\n",
      "('ZYDUSWELL', 11556.099999999999)\n",
      "('ZUARIGLOB', 1681.6000000000001)\n"
     ]
    }
   ],
   "source": [
    "elements = avgs_desc.take(3)\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_desc.saveAsTextFile(\"./averages-3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
