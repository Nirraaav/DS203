{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E10-2\n",
    "\n",
    "Nirav Bhattad (23B3307)\n",
    "\n",
    "#### This Notebook illustrates the use of SPARK Dataframe functions to process nsedata.csv\n",
    "- Review <b>Part-1</b> to understand the code by referring to SPARK documentation.\n",
    "- Add your comment to each cell, to explain its purpose\n",
    "- Add code / create additional cells for debugging purpose, and comment them too \n",
    "- Write SPARK code to solve the problem stated in <b>Part-2</b> (do not use the createTempView function in your solution!)\n",
    "\n",
    "<b>Submission</b>\n",
    "- Create and upload a PDF of this Notebook.\n",
    "- <b> BEFORE CONVERTING TO PDF ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS </b>.\n",
    "- Short debug outputs of up to 5 lines are acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # here we are importing findspark module to locate the spark in the system\n",
    "findspark.init() # here we are initializing the spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # here we are importing pyspark module\n",
    "from pyspark.sql.types import * # here we are importing all the classes from the module\n",
    "from pyspark.sql import functions as F # here we are importing functions from the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E10-2\") # here we are creating a spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pyspark.sql.SparkSession(sc) # here we are creating a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = ss.read # here we are reading the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('SYMBOL', StringType(), True), StructField('SERIES', StringType(), True), StructField('OPEN', DoubleType(), True), StructField('HIGH', DoubleType(), True), StructField('LOW', DoubleType(), True), StructField('CLOSE', DoubleType(), True), StructField('LAST', DoubleType(), True), StructField('PREVCLOSE', DoubleType(), True), StructField('TOTTRDQTY', LongType(), True), StructField('TOTTRDVAL', DoubleType(), True), StructField('TIMESTAMP', StringType(), True), StructField('ADDNL', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaStruct = StructType()\n",
    "schemaStruct.add(\"SYMBOL\", StringType(), True)\n",
    "schemaStruct.add(\"SERIES\", StringType(), True)\n",
    "schemaStruct.add(\"OPEN\", DoubleType(), True)\n",
    "schemaStruct.add(\"HIGH\", DoubleType(), True)\n",
    "schemaStruct.add(\"LOW\", DoubleType(), True)\n",
    "schemaStruct.add(\"CLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"LAST\", DoubleType(), True)\n",
    "schemaStruct.add(\"PREVCLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"TOTTRDQTY\", LongType(), True)\n",
    "schemaStruct.add(\"TOTTRDVAL\", DoubleType(), True)\n",
    "schemaStruct.add(\"TIMESTAMP\", StringType(), True)\n",
    "schemaStruct.add(\"ADDNL\", StringType(), True)\n",
    "\n",
    "# here we are reading the data from the file, and we are providing the schema to the data as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfr.csv(\"./nsedata.csv\", schema=schemaStruct, header=True) # here we are reading the data from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Basics : Using SPARK for analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_from_df(company_code):\n",
    "    \"\"\"\n",
    "    This function takes a company code as input and returns a subset of the dataframe with the columns OPEN, HIGH, LOW, CLOSE and TIMESTAMP for the given company code.\n",
    "\n",
    "    Parameters:\n",
    "        company_code: str: A string representing the company code.\n",
    "\n",
    "    Returns:\n",
    "        df_subset: DataFrame: A subset of the dataframe with the columns OPEN, HIGH, LOW, CLOSE and TIMESTAMP for the given company code.\n",
    "    \"\"\"\n",
    "    tcode = company_code.lower()\n",
    "    df_subset = df.select(\\\n",
    "                    F.col(\"OPEN\").alias(\"OPEN_\" + tcode),\\\n",
    "                    F.col(\"HIGH\").alias(\"HIGH_\"+ tcode),\\\n",
    "                    F.col(\"LOW\").alias(\"LOW_\"+ tcode),\\\n",
    "                    F.col(\"CLOSE\").alias(\"CLOSE_\" + tcode),\\\n",
    "                    F.col(\"TIMESTAMP\")).\\\n",
    "                    where(F.col(\"SYMBOL\") == company_code)\n",
    "    return(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need to use the alias function, above? What happens if we do not alias / rename the columns?\n",
    "# Answer: We need to use alias function to rename the columns, because if we do not rename the columns, then the columns will have the same name as the original dataframe, and it will be difficult to differentiate between the columns of the original dataframe and the subset dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infy = create_subset_from_df(\"INFY\")\n",
    "df_infy.show(5)\n",
    "df_infy.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcs = create_subset_from_df(\"TCS\")\n",
    "df_tcs.show(5)\n",
    "df_tcs.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 20:51:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 9:==>                                                      (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|02-FEB-2012|   1148.0|    2757.0|\n",
      "|01-AUG-2013|   1815.4|   2974.65|\n",
      "|01-DEC-2014|  2692.95|   4349.85|\n",
      "|01-OCT-2014|   2775.6|    3847.3|\n",
      "|01-JUL-2013|  1492.35|    2451.0|\n",
      "+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join = df_tcs.join(df_infy,\"TIMESTAMP\").select(\"TIMESTAMP\",\"CLOSE_tcs\",\"CLOSE_infy\")\n",
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         PriceDiff|\n",
      "+-------+------------------+\n",
      "|  count|              1025|\n",
      "|   mean|1163.6446341463422|\n",
      "| stddev| 366.9897015322771|\n",
      "|    min|150.95000000000027|\n",
      "|    max|            1804.9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.select(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]).alias(\"PriceDiff\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                       (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+\n",
      "|  TIMESTAMP|CLOSE_tcs|CLOSE_infy|\n",
      "+-----------+---------+----------+\n",
      "|10-FEB-2015|  2441.15|    2278.3|\n",
      "|11-FEB-2015|   2459.9|   2284.85|\n",
      "|12-FEB-2015|  2462.15|    2311.2|\n",
      "+-----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.filter(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]) < 180).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "df1 = df.withColumn(\"TIMESTAMP2\", date_format(to_date(col(\"TIMESTAMP\"), \"dd-MMM-yyyy\"), \"yyyy-MM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 20:52:02 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 12\n",
      "CSV file: file:///home/nirav24/Desktop/Academia/IITB%20Courses/Sem%203/DS203/Exercise-10/nsedata.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "|    SYMBOL|SERIES|  OPEN|   HIGH|   LOW| CLOSE|  LAST|PREVCLOSE|TOTTRDQTY|    TOTTRDVAL|  TIMESTAMP|ADDNL|TIMESTAMP2|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "| 20MICRONS|    EQ| 37.75|  37.75| 36.35| 37.45|  37.3|    37.15|    38638|    1420968.1|01-APR-2011|    0|   2011-04|\n",
      "|3IINFOTECH|    EQ| 43.75|   45.3| 43.75|  44.9|  44.8|    43.85|  1239690|5.531120435E7|01-APR-2011|    0|   2011-04|\n",
      "|   3MINDIA|    EQ|3374.0|3439.95|3338.0|3397.5|3400.0|   3364.7|      871|   2941547.35|01-APR-2011|    0|   2011-04|\n",
      "|    A2ZMES|    EQ| 281.8| 294.45| 279.8| 289.2| 287.2|    281.3|   140643| 4.02640755E7|01-APR-2011|    0|   2011-04|\n",
      "|AARTIDRUGS|    EQ| 127.0|  132.0|126.55| 131.3| 130.6|    127.6|     2972|     384468.2|01-APR-2011|    0|   2011-04|\n",
      "+----------+------+------+-------+------+------+------+---------+---------+-------------+-----------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_t1 = df1.groupBy(\"SYMBOL\",\"TIMESTAMP2\").agg(F.min(\"OPEN\"), F.max(\"OPEN\"), F.avg(\"OPEN\"),\\\n",
    "                                       F.stddev(\"OPEN\"), F.count(\"OPEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================>                                 (8 + 12) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|    SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|  AREVAT&D|   2011-04|   246.15|   292.95|274.73055555555555| 12.60998832495714|         18|\n",
      "| CHEMPLAST|   2011-04|      6.3|     8.25| 7.172222222222223|  0.55709916273872|         18|\n",
      "|FIRSTLEASE|   2011-04|     68.3|   106.05| 93.05277777777776| 10.68782254033041|         18|\n",
      "|    FORTIS|   2011-04|    152.0|    163.4|159.50833333333333|2.7349723087102285|         18|\n",
      "| GOLDINFRA|   2011-04|    16.85|    20.15|            17.925|0.7857648952379039|         18|\n",
      "+----------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_t1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2 = df_t1.sort(F.asc(\"SYMBOL\"), F.asc(\"TIMESTAMP2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:============================>                          (11 + 10) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|   SYMBOL|TIMESTAMP2|min(OPEN)|max(OPEN)|         avg(OPEN)|      stddev(OPEN)|count(OPEN)|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "|20MICRONS|   2010-08|     51.6|     54.0| 52.81666666666667|0.9266876496425305|          9|\n",
      "|20MICRONS|   2010-09|     54.9|     64.3| 59.11428571428571| 2.514614426564381|         21|\n",
      "|20MICRONS|   2010-10|    55.05|     60.0|57.166666666666664|1.3035848009751174|         21|\n",
      "|20MICRONS|   2010-11|     53.6|    61.75| 55.98809523809524|2.2001650370997607|         21|\n",
      "|20MICRONS|   2010-12|     38.8|     61.0| 45.66590909090908| 5.796599708606603|         22|\n",
      "+---------+----------+---------+---------+------------------+------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_t2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Uncomment the following statement to generate the output, and analyze it\n",
    "# Write your observations in the next cell\n",
    "\n",
    "df_t2.write.csv(\"monthly_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observation related to monthly_stats\n",
    "# The monthly_stats.csv file contains the monthly statistics of the stock market data, and it contains the minimum, maximum, average, standard deviation and count of the opening price of the stocks for each company for each month.\n",
    "# it is sorted in order by the company code and the timestamp2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>SPARK based solutions for stock analysis and portfolio management: An Example</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Based on equity (EQ) data contained in nsedata.csv, you are tasked with the responsibility to identify a set of 10 stocks to invest in based on the following steps:\n",
    "\n",
    "- You have to process the data for one entire year, and then make investment decisions for the following year. You can shoose 2012 as the past year and make recommendations for 2013.\n",
    "- Assume that you are doing this analysis on Jan 1, 2013. \n",
    "- You are required to draw up an initial list of 10 stocks based on the following preliminary analysis: \n",
    "    - The stocks should be liquid. That is, they should be traded in large volumes almost every day and the trading volume should be high.\n",
    "    - You have to filter those stocks that have shown maximum overall growth over the past year. The hope is that they will continue to grow in the future.\n",
    "- Select 5 pairs of stocks from these filtered stocks based on the following further analysis.\n",
    "    - You should ensure that volatility and negative market movements in the coming year will not adversely affect the total investment, substantially.\n",
    "    - One way to achieve this involves selecting <b>stock pairs that are negatively correlated</b>, so that if one stock loses value its partner will most likely gain value - thereby reducing the overall impact of fall in stock prices. As all these stocks are high growth stocks, anyway, the expectation is that there also will be overall growth of the portfolio. \n",
    "    - Purchase 1 unit of each of these stock pairs on the first trading day of the next year (i.e. 2013)\n",
    "- Once you have selected the 5 pairs and made the above investments, you should further do the following\n",
    "    - Report the performance of your portfolio as on 31/12/2013 (or the nearest traded date, if 31/12/2013 was a non traded day) in terms of the:\n",
    "        - Overall growth of your portfolio\n",
    "        - Report which stocks in your portfolio grew in value, which of them reduced in value, an whether the pairing strategy worked.\n",
    "        - How did the overall market perform during the same period? This can be assessed as follows:\n",
    "            - If you had blindly selected 1 stock each of the top 25 highly traded, high growth stocks, what would have been the performance of this portfolio\n",
    "            - How did the implemented strategy of selecting highly traded, high growth stocks, but in pairs having <b>negative correlation</b>, perform in comparion? Did the strategy work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some suggested steps to solve the problem\n",
    "\n",
    "# First of all select only EQUITY related data\n",
    "# Create a dataframe of stocks that have traded in during the year 2012\n",
    "# Find out the average total traded quantity of each of these stocks\n",
    "# Identify stocks that high trade volumes: average daily volume ranging between 5L and 10L\n",
    "# Find out the price difference in each of these stocks between the 'last traded day of 2012' and 'first traded day of 2012'\n",
    "# Sort the stocks in descending order using traded quantity and price difference as the criteria\n",
    "# Select the top 10 stocks for further analysis\n",
    "# Create a new dataframe containing pairs of stocks traded on the same day \n",
    "#   - join the selected stocks by using the criteria that stock names in the resulting dataframe are different\n",
    "# Sort the dataframe in ascending order\n",
    "# Establish the criteria for selecting the final pairs of stocks, and select them\n",
    "# Calculate your total investment value\n",
    "# ... likewise state and complete the rest of the steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2012'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012_avgqty = df_2012.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "df_2012_avgqty.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df_2012_avgqty.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t1 = top10.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "df_for_corr = t3.join(t4,\"TIMESTAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+------+\n",
      "|  TIMESTAMP|      S1|Close1|    S2|Close2|\n",
      "+-----------+--------+------+------+------+\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  STER| 124.1|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|RENUKA| 38.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  NHPC| 20.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|   ITC|199.05|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|  IDFC|131.45|\n",
      "+-----------+--------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_for_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(S1='IDFC', S2='NHPC'),\n",
       " Row(S1='RENUKA', S2='IDFC'),\n",
       " Row(S1='ALOKTEXT', S2='ASHOKLEY'),\n",
       " Row(S1='STER', S2='ITC'),\n",
       " Row(S1='RENUKA', S2='STER'),\n",
       " Row(S1='HINDALCO', S2='IDFC'),\n",
       " Row(S1='ITC', S2='ALOKTEXT'),\n",
       " Row(S1='ALOKTEXT', S2='NHPC'),\n",
       " Row(S1='DLF', S2='ALOKTEXT'),\n",
       " Row(S1='GMRINFRA', S2='ITC')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrklist = df_for_corr.select(\"S1\",\"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "wrklist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(wrklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TAKES QUITE SOME TIME TO EXECUTE - BE PATIENT!\n",
    "tcorr = []\n",
    "tlen = len(wrklist)\n",
    "for i in range(tlen):\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\",\"Close2\")\n",
    "    tcorr.append([s1,s2,corr])\n",
    "    if((i+1)%10 ==0):\n",
    "        print(f\"Processed: {i+1} of {tlen}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+\n",
      "| Symbol1| Symbol2|      Corr|\n",
      "+--------+--------+----------+\n",
      "|    IDFC|    NHPC| 0.7452768|\n",
      "|  RENUKA|    IDFC| 0.2969912|\n",
      "|ALOKTEXT|ASHOKLEY|0.47407645|\n",
      "|    STER|     ITC|-0.3065829|\n",
      "|  RENUKA|    STER| 0.6944879|\n",
      "+--------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "rdd = sc.parallelize(tcorr)\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "df_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr_neg = df_corr.filter(F.col(\"Corr\") <= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").asc())\n",
    "df_corr_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+\n",
      "| Symbol1| Symbol2|         Corr|\n",
      "+--------+--------+-------------+\n",
      "|     ITC|ALOKTEXT|  -0.90314275|\n",
      "|GMRINFRA|     ITC|   -0.7135044|\n",
      "|    IDFC|ALOKTEXT|   -0.6409445|\n",
      "|HINDALCO|     ITC|  -0.62534785|\n",
      "|ALOKTEXT|    NHPC|  -0.33097458|\n",
      "|     ITC|ASHOKLEY|   -0.3144176|\n",
      "|    STER|     ITC|   -0.3065829|\n",
      "|GMRINFRA|    IDFC|  -0.28986531|\n",
      "|     ITC|  RENUKA|  -0.21256758|\n",
      "|     DLF|ALOKTEXT|  -0.16802602|\n",
      "|GMRINFRA|    NHPC| -0.048641354|\n",
      "|HINDALCO|    IDFC|-0.0068381117|\n",
      "+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_corr_neg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 887:==>                                                    (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-JAN-2013 31-DEC-2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2013 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2013'\")\n",
    "\n",
    "first_day_2013 = (df_2013.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2013'\").distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2013 = (df_2013.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2013'\").distinct().orderBy(\"TIMESTAMP\",ascending=False).first())[0]\n",
    "\n",
    "print(first_day_2013,last_day_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    loc_price = df_2013.where(F.col(\"TIMESTAMP\")==loc_date).where(F.col(\"SYMBOL\")==loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    return((loc_price)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected stocks, based on the analysis\n",
    "# |     ITC|ALOKTEXT|  -0.90314275|\n",
    "# |GMRINFRA|     ITC|   -0.7135044|\n",
    "# |    IDFC|ALOKTEXT|   -0.6409445|\n",
    "# |HINDALCO|     ITC|  -0.62534785|\n",
    "# |ALOKTEXT|    NHPC|  -0.33097458|\n",
    "\n",
    "stock_list = [\"ITC\",\"ALOKTEXT\",\"GMRINFRA\",\"IDFC\",\"HINDALCO\",\"NHPC\"]\n",
    "multiplier = [3,3,1,1,1,1]\n",
    "\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "for the_stock,the_multiplier in zip(stock_list,multiplier):\n",
    "    first_day_price = get_price_on_day(the_stock,first_day_2013)\n",
    "    last_day_price  = get_price_on_day(the_stock,last_day_2013)\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    prices.append([the_stock,first_day_price,last_day_price,diff,total_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ITC', 287.25, 321.85, 34.60000000000002, 103.80000000000007],\n",
       " ['ALOKTEXT', 11.35, 8.45, -2.9000000000000004, -8.700000000000001],\n",
       " ['GMRINFRA', 20.3, 24.8, 4.5, 4.5],\n",
       " ['IDFC', 173.65, 109.6, -64.05000000000001, -64.05000000000001],\n",
       " ['HINDALCO', 134.15, 122.6, -11.550000000000011, -11.550000000000011],\n",
       " ['NHPC', 25.35, 19.55, -5.800000000000001, -5.800000000000001]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249.25 18.200000000000042\n"
     ]
    }
   ],
   "source": [
    "print(total_investment, total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25 = df_2012_avgqty.limit(25)\n",
    "t1 = top25.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "t2 = df_2013.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "first_day_overall = t2.where(F.col(\"TIMESTAMP\")==first_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "last_day_overall = t2.where(F.col(\"TIMESTAMP\")==last_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "total_profit_overall = last_day_overall[0][0] - first_day_overall[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount: 5119.3 invested on the first trading day of 2013\n",
      "has a value: 4226.45 on the last trading day of 2013\n",
      "The profit/loss is : -892.85 corresponding to -17.44%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Amount: {first_day_overall[0][0]} invested on the first trading day of 2013\\n\\\n",
    "has a value: {last_day_overall[0][0]} on the last trading day of 2013\\n\\\n",
    "The profit/loss is : {total_profit_overall:.2f} corresponding to {total_profit_overall/first_day_overall[0][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Performance of the strategy</b>\n",
    "- If we had invested in all the top 25 stocks, without implementing the negative correlation strategy, \n",
    "There would have been a loss of 892 on an investment of 5119 (17.5% loss)\n",
    "- As against that, by implementing the 'select based on negative correlation' strategy, \n",
    "a profit of 18.2 on an investment of 1249 (1.5% profit) has been achieved\n",
    "- In conclusion, the strategy has definitely prevented portfolio value loss during a bad year. It has, in fact, preserved capital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 2 : <b>Problem to solve</b></b>\n",
    "1. Which of the following is better, if you have 10 Lakhs to invest for a year: \n",
    "    - identify 5 top performing stocks of the previous year and invest in them, or\n",
    "    - Spread your investment across a basket of 25 stocks, with investments equally distributed among them\n",
    "    - Employing strategies like 'negative correlation' to select your stocks\n",
    "    - What if you use 'positive correlation' instead, carry out analysis to understand the portfolio's performance?\n",
    "2. Do your analysis over multiple years (2011-2012, 2012-2013, etc.) to make your final recommendations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the all of the distinct series\n",
    "df.select(\"SERIES\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 performing stocks in 2011:\n",
      "['AVTNPL', 'ALFALAVAL', 'INSECTICID', 'UTVSOF', 'VSTIND', 'BHARATRAS', 'PAGEIND', 'GUJFLUORO', 'IFBAGRO', 'TTKPRESTIG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|    SYMBOL|    Percent_Change|\n",
      "+----------+------------------+\n",
      "|    AVTNPL|131.03164817098232|\n",
      "| ALFALAVAL| 79.80894839973875|\n",
      "|INSECTICID| 76.00891861761426|\n",
      "|    UTVSOF|  73.5927665987058|\n",
      "|    VSTIND| 72.42339832869081|\n",
      "| BHARATRAS| 60.31105990783411|\n",
      "|   PAGEIND| 57.10582444626744|\n",
      "| GUJFLUORO| 55.81867388362652|\n",
      "|   IFBAGRO| 54.09927495817066|\n",
      "|TTKPRESTIG| 52.88052074461614|\n",
      "+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: AVTNPL, First Price: 285.55, Last Price: 35.05, Change: -250.50\n",
      "Stock: INSECTICID, First Price: 394.85, Last Price: 408.65, Change: 13.80\n",
      "Stock: VSTIND, First Price: 1071.70, Last Price: 1950.05, Change: 878.35\n",
      "Stock: BHARATRAS, First Price: 142.60, Last Price: 171.55, Change: 28.95\n",
      "Stock: PAGEIND, First Price: 2400.15, Last Price: 3424.35, Change: 1024.20\n",
      "Total Investment: 4294.85\n",
      "Total Profit: 1694.80\n",
      "Profit Percentage: 39.46%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Step 1: Filter and preprocess data for 2011 and 2012\n",
    "df_2011 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "df_2012 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2012'\")\n",
    "\n",
    "# Step 2: Calculate yearly performance for 2011\n",
    "first_day_2011 = (df_2011.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2011'\")\n",
    "                  .distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2011 = (df_2011.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2011'\")\n",
    "                 .distinct().orderBy(\"TIMESTAMP\", ascending=False).first())[0]\n",
    "\n",
    "# Rename CLOSE column in first_prices_2011\n",
    "first_prices_2011 = df_2011.filter(F.col(\"TIMESTAMP\") == first_day_2011) \\\n",
    "    .select(F.col(\"SYMBOL\"), F.col(\"CLOSE\").alias(\"CLOSE_x\"))\n",
    "\n",
    "# Rename CLOSE column in last_prices_2011\n",
    "last_prices_2011 = df_2011.filter(F.col(\"TIMESTAMP\") == last_day_2011) \\\n",
    "    .select(F.col(\"SYMBOL\"), F.col(\"CLOSE\").alias(\"CLOSE_y\"))\n",
    "\n",
    "# Perform the join\n",
    "performance_2011 = first_prices_2011.join(last_prices_2011, \"SYMBOL\") \\\n",
    "    .withColumn(\"Percent_Change\", ((F.col(\"CLOSE_y\") - F.col(\"CLOSE_x\")) / F.col(\"CLOSE_x\")) * 100) \\\n",
    "    .select(\"SYMBOL\", \"Percent_Change\") \\\n",
    "    .orderBy(F.col(\"Percent_Change\").desc())\n",
    "\n",
    "# Select top 5 performing stocks\n",
    "top_stocks_2011 = performance_2011.limit(10).select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(\"Top 10 performing stocks in 2011:\")\n",
    "print(top_stocks_2011)\n",
    "performance_2011.show(10)\n",
    "\n",
    "# Step 3: Simulate investments in 2012\n",
    "first_day_2012 = (df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2012'\")\n",
    "                  .distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2012 = (df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2012'\")\n",
    "                 .distinct().orderBy(\"TIMESTAMP\", ascending=False).first())[0]\n",
    "\n",
    "# Function to get price on a specific day\n",
    "def get_price_on_day(stock, date):\n",
    "    price = df_2012.filter(F.col(\"TIMESTAMP\") == date).filter(F.col(\"SYMBOL\") == stock).select(\"CLOSE\").collect()\n",
    "    return price[0][0] if price else None\n",
    "\n",
    "# Simulate investments\n",
    "investment_results = []\n",
    "total_investment = 0\n",
    "total_profit = 0\n",
    "\n",
    "for stock in top_stocks_2011:\n",
    "    first_price = get_price_on_day(stock, first_day_2012)\n",
    "    last_price = get_price_on_day(stock, last_day_2012)\n",
    "    \n",
    "    if first_price and last_price:\n",
    "        diff = last_price - first_price\n",
    "        investment_results.append([stock, first_price, last_price, diff])\n",
    "        total_investment += first_price\n",
    "        total_profit += diff\n",
    "\n",
    "    if(len(investment_results) == 5):\n",
    "        break\n",
    "\n",
    "# Display results\n",
    "for result in investment_results:\n",
    "    print(f\"Stock: {result[0]}, First Price: {result[1]:.2f}, Last Price: {result[2]:.2f}, Change: {result[3]:.2f}\")\n",
    "\n",
    "print(f\"Total Investment: {total_investment:.2f}\")\n",
    "print(f\"Total Profit: {total_profit:.2f}\")\n",
    "print(f\"Profit Percentage: {total_profit / total_investment * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 performing stocks in 2011:\n",
      "['AVTNPL', 'ALFALAVAL', 'INSECTICID', 'UTVSOF', 'VSTIND', 'BHARATRAS', 'PAGEIND', 'GUJFLUORO', 'IFBAGRO', 'TTKPRESTIG', 'VISASTEEL', 'INEABS', 'TATACOFFEE', 'GITANJALI', 'BATAINDIA', 'SURANAIND', 'BLUEDART', 'GRUH', 'RAJTV', 'GRAVITA', 'AMTEKINDIA', 'REPRO', 'SOLARINDS', 'AJANTPHARM', 'KAJARIACER', 'BRFL', '20MICRONS', 'LAOPALA', 'HINDUNILVR', 'RELGOLD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|    SYMBOL|    Percent_Change|\n",
      "+----------+------------------+\n",
      "|    AVTNPL|131.03164817098232|\n",
      "| ALFALAVAL| 79.80894839973875|\n",
      "|INSECTICID| 76.00891861761426|\n",
      "|    UTVSOF|  73.5927665987058|\n",
      "|    VSTIND| 72.42339832869081|\n",
      "| BHARATRAS| 60.31105990783411|\n",
      "|   PAGEIND| 57.10582444626744|\n",
      "| GUJFLUORO| 55.81867388362652|\n",
      "|   IFBAGRO| 54.09927495817066|\n",
      "|TTKPRESTIG| 52.88052074461614|\n",
      "| VISASTEEL|51.574803149606296|\n",
      "|    INEABS| 48.62431849879548|\n",
      "|TATACOFFEE| 48.42865508491119|\n",
      "| GITANJALI| 43.04337520739512|\n",
      "| BATAINDIA| 42.08551132555956|\n",
      "| SURANAIND|42.078011736278924|\n",
      "|  BLUEDART| 41.31220709663021|\n",
      "|      GRUH|  39.2198404785643|\n",
      "|     RAJTV| 39.00000000000001|\n",
      "|   GRAVITA| 38.42215057841836|\n",
      "|AMTEKINDIA| 37.87110789283129|\n",
      "|     REPRO|37.643555933645274|\n",
      "| SOLARINDS|  37.4910007199424|\n",
      "|AJANTPHARM|  37.3419176822258|\n",
      "|KAJARIACER| 35.65629228687414|\n",
      "|      BRFL| 35.55555555555555|\n",
      "| 20MICRONS| 33.44051446945338|\n",
      "|   LAOPALA| 30.16949152542373|\n",
      "|HINDUNILVR|30.097397413380172|\n",
      "|   RELGOLD|29.900636942675167|\n",
      "+----------+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock: AVTNPL, First Price: 285.55, Last Price: 35.05, Change: -250.50\n",
      "Stock: INSECTICID, First Price: 394.85, Last Price: 408.65, Change: 13.80\n",
      "Stock: VSTIND, First Price: 1071.70, Last Price: 1950.05, Change: 878.35\n",
      "Stock: BHARATRAS, First Price: 142.60, Last Price: 171.55, Change: 28.95\n",
      "Stock: PAGEIND, First Price: 2400.15, Last Price: 3424.35, Change: 1024.20\n",
      "Stock: GUJFLUORO, First Price: 356.05, Last Price: 333.00, Change: -23.05\n",
      "Stock: IFBAGRO, First Price: 138.40, Last Price: 183.20, Change: 44.80\n",
      "Stock: TTKPRESTIG, First Price: 2500.90, Last Price: 3379.00, Change: 878.10\n",
      "Stock: VISASTEEL, First Price: 57.90, Last Price: 47.60, Change: -10.30\n",
      "Stock: TATACOFFEE, First Price: 761.75, Last Price: 1407.65, Change: 645.90\n",
      "Stock: GITANJALI, First Price: 312.25, Last Price: 531.65, Change: 219.40\n",
      "Stock: BATAINDIA, First Price: 529.95, Last Price: 866.95, Change: 337.00\n",
      "Stock: SURANAIND, First Price: 418.00, Last Price: 139.20, Change: -278.80\n",
      "Stock: BLUEDART, First Price: 1590.20, Last Price: 2047.55, Change: 457.35\n",
      "Stock: GRUH, First Price: 548.45, Last Price: 237.45, Change: -311.00\n",
      "Stock: RAJTV, First Price: 83.70, Last Price: 195.45, Change: 111.75\n",
      "Stock: GRAVITA, First Price: 402.20, Last Price: 183.35, Change: -218.85\n",
      "Stock: AMTEKINDIA, First Price: 97.00, Last Price: 106.05, Change: 9.05\n",
      "Stock: REPRO, First Price: 163.90, Last Price: 218.95, Change: 55.05\n",
      "Stock: SOLARINDS, First Price: 762.65, Last Price: 958.45, Change: 195.80\n",
      "Stock: AJANTPHARM, First Price: 301.75, Last Price: 382.15, Change: 80.40\n",
      "Stock: KAJARIACER, First Price: 96.60, Last Price: 231.95, Change: 135.35\n",
      "Stock: BRFL, First Price: 270.95, Last Price: 243.75, Change: -27.20\n",
      "Stock: 20MICRONS, First Price: 62.50, Last Price: 156.95, Change: 94.45\n",
      "Stock: LAOPALA, First Price: 96.45, Last Price: 268.85, Change: 172.40\n",
      "25\n",
      "Total Investment: 1000000.00\n",
      "Total Profit: 344879.68\n",
      "Profit Percentage: 34.49%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Filter and preprocess data for 2011 and 2012\n",
    "df_2011 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "df_2012 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2012'\")\n",
    "\n",
    "# Step 2: Calculate yearly performance for 2011\n",
    "first_day_2011 = (df_2011.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2011'\")\n",
    "                  .distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2011 = (df_2011.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2011'\")\n",
    "                 .distinct().orderBy(\"TIMESTAMP\", ascending=False).first())[0]\n",
    "\n",
    "# Rename CLOSE column in first_prices_2011\n",
    "first_prices_2011 = df_2011.filter(F.col(\"TIMESTAMP\") == first_day_2011) \\\n",
    "    .select(F.col(\"SYMBOL\"), F.col(\"CLOSE\").alias(\"CLOSE_x\"))\n",
    "\n",
    "# Rename CLOSE column in last_prices_2011\n",
    "last_prices_2011 = df_2011.filter(F.col(\"TIMESTAMP\") == last_day_2011) \\\n",
    "    .select(F.col(\"SYMBOL\"), F.col(\"CLOSE\").alias(\"CLOSE_y\"))\n",
    "\n",
    "# Perform the join\n",
    "performance_2011 = first_prices_2011.join(last_prices_2011, \"SYMBOL\") \\\n",
    "    .withColumn(\"Percent_Change\", ((F.col(\"CLOSE_y\") - F.col(\"CLOSE_x\")) / F.col(\"CLOSE_x\")) * 100) \\\n",
    "    .select(\"SYMBOL\", \"Percent_Change\") \\\n",
    "    .orderBy(F.col(\"Percent_Change\").desc())\n",
    "\n",
    "# Select top 25 performing stocks\n",
    "top_stocks_2011 = performance_2011.limit(30).select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(\"Top 30 performing stocks in 2011:\")\n",
    "print(top_stocks_2011)\n",
    "performance_2011.show(30)\n",
    "\n",
    "# Step 3: Simulate investments in 2012\n",
    "first_day_2012 = (df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2012'\")\n",
    "                  .distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2012 = (df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2012'\")\n",
    "                 .distinct().orderBy(\"TIMESTAMP\", ascending=False).first())[0]\n",
    "\n",
    "# Function to get price on a specific day\n",
    "def get_price_on_day(stock, date):\n",
    "    price = df_2012.filter(F.col(\"TIMESTAMP\") == date).filter(F.col(\"SYMBOL\") == stock).select(\"CLOSE\").collect()\n",
    "    return price[0][0] if price else None\n",
    "\n",
    "# Simulate investments\n",
    "investment_results = []\n",
    "total_investment = 0\n",
    "total_profit = 0\n",
    "\n",
    "# Total money to invest equally across 25 stocks\n",
    "total_money = 1000000  # Example total investment amount (e.g., 1 million)\n",
    "investment_per_stock = total_money / 25  # Equal investment for each of the 25 stocks\n",
    "\n",
    "# Loop through each stock in the top 25\n",
    "for stock in top_stocks_2011:\n",
    "    first_price = get_price_on_day(stock, first_day_2012)\n",
    "    last_price = get_price_on_day(stock, last_day_2012)\n",
    "    \n",
    "    if first_price and last_price:\n",
    "        diff = last_price - first_price\n",
    "        investment_results.append([stock, first_price, last_price, diff])\n",
    "        total_investment += investment_per_stock\n",
    "        total_profit += diff * (investment_per_stock / first_price)\n",
    "\n",
    "    if len(investment_results) == 25:\n",
    "        break\n",
    "\n",
    "# Display results for each stock in the basket\n",
    "for result in investment_results:\n",
    "    print(f\"Stock: {result[0]}, First Price: {result[1]:.2f}, Last Price: {result[2]:.2f}, Change: {result[3]:.2f}\")\n",
    "\n",
    "print(len(investment_results))\n",
    "print(f\"Total Investment: {total_investment:.2f}\")\n",
    "print(f\"Total Profit: {total_profit:.2f}\")\n",
    "print(f\"Profit Percentage: {total_profit / total_investment * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|    SYMBOL|   avg(TOTTRDQTY)|\n",
      "+----------+-----------------+\n",
      "|  ALOKTEXT|8677144.275303643|\n",
      "|    GVKPIL|8149384.765182186|\n",
      "|  HINDALCO|7992351.906882592|\n",
      "|      RCOM|7713584.137651822|\n",
      "|    RENUKA|7459392.910931174|\n",
      "|SHREEASHTA|7339390.076923077|\n",
      "|       ITC|7325373.246963562|\n",
      "|      IDFC|7102852.137651822|\n",
      "|      HDIL|6585712.372469636|\n",
      "|TATAMOTORS|6425309.267206478|\n",
      "+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----------+------+\n",
      "|  TIMESTAMP|      S1|Close1|        S2|Close2|\n",
      "+-----------+--------+------+----------+------+\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|TATAMOTORS|246.45|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|SHREEASHTA|   4.0|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|    RENUKA| 38.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|      RCOM|  96.7|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|       ITC|199.05|\n",
      "+-----------+--------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|   Symbol1|   Symbol2|       Corr|\n",
      "+----------+----------+-----------+\n",
      "|       ITC|SHREEASHTA| -0.5946509|\n",
      "|    GVKPIL|      IDFC|0.025669474|\n",
      "|    RENUKA|      IDFC|  0.2969912|\n",
      "|      RCOM|    GVKPIL|  0.8123094|\n",
      "|TATAMOTORS|       ITC| 0.25560105|\n",
      "+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------+-------------+\n",
      "|   Symbol1|   Symbol2|         Corr|\n",
      "+----------+----------+-------------+\n",
      "|       ITC|  ALOKTEXT|  -0.90314275|\n",
      "|       ITC|      RCOM|   -0.6903315|\n",
      "|      IDFC|  ALOKTEXT|   -0.6409445|\n",
      "|  HINDALCO|       ITC|  -0.62534785|\n",
      "|       ITC|SHREEASHTA|   -0.5946509|\n",
      "|    GVKPIL|       ITC|  -0.50507504|\n",
      "|  ALOKTEXT|      HDIL|   -0.3032723|\n",
      "|       ITC|    RENUKA|  -0.21256758|\n",
      "|SHREEASHTA|      IDFC|  -0.14311746|\n",
      "|      RCOM|      IDFC|  -0.11569301|\n",
      "|  ALOKTEXT|TATAMOTORS|  -0.09804849|\n",
      "|SHREEASHTA|TATAMOTORS|-0.0090005705|\n",
      "|  HINDALCO|      IDFC|-0.0068381117|\n",
      "+----------+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day of 2012: 02-JAN-2012, Last day of 2012: 31-DEC-2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Step 1: Filter for 2011 data to train the model\n",
    "df_2011 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "df_2011_avgqty = df_2011.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "df_2011_avgqty.show(10)\n",
    "\n",
    "# Step 2: Select the top 10 stocks for training\n",
    "top10_2011 = df_2011_avgqty.limit(10)\n",
    "t1 = top10_2011.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Step 3: Filter the data for 2012 testing based on selected stocks\n",
    "df_2012 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2012'\")\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Step 4: Prepare for correlation calculation (join data for pairwise correlations)\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "df_for_corr = t3.join(t4, \"TIMESTAMP\")\n",
    "df_for_corr.show(5)\n",
    "\n",
    "# Step 5: Get distinct pairs for correlation calculation\n",
    "wrklist = df_for_corr.select(\"S1\", \"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "\n",
    "# Step 6: Calculate correlation for each pair\n",
    "tcorr = []\n",
    "tlen = len(wrklist)\n",
    "for i in range(tlen):\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\", \"Close2\")\n",
    "    tcorr.append([s1, s2, corr])\n",
    "    # if ((i + 1) % 10 == 0):\n",
    "        # print(f\"Processed: {i + 1} of {tlen}\", end='')\n",
    "\n",
    "# Step 7: Create a DataFrame for correlations\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "rdd = sc.parallelize(tcorr)\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "df_corr.show(5)\n",
    "\n",
    "# Step 8: Filter negative correlations\n",
    "df_corr_neg = df_corr.filter(F.col(\"Corr\") <= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").asc())\n",
    "df_corr_neg.count()\n",
    "\n",
    "df_corr_neg.show()\n",
    "\n",
    "# Step 9: Get first and last days of 2012 for selected stocks (testing period)\n",
    "df_2012_first_day = df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2012'\").distinct().orderBy(\"TIMESTAMP\").first()[0]\n",
    "df_2012_last_day = df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2012'\").distinct().orderBy(\"TIMESTAMP\", ascending=False).first()[0]\n",
    "\n",
    "print(f\"First day of 2012: {df_2012_first_day}, Last day of 2012: {df_2012_last_day}\")\n",
    "\n",
    "# Step 10: Function to get price on a specific day\n",
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    loc_price = df_2012.where(F.col(\"TIMESTAMP\") == loc_date).where(F.col(\"SYMBOL\") == loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    return (loc_price)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Summary:\n",
      "Total Investment: 886.80\n",
      "Total Profit: 474.80\n",
      "Profit Percentage: 53.54%\n",
      "Stock: ITC, First Price: 198.65, Last Price: 286.80, Change: 88.15, Total Profit: 176.30\n",
      "Stock: GVKPIL, First Price: 12.40, Last Price: 13.55, Change: 1.15, Total Profit: 2.30\n",
      "Stock: IDFC, First Price: 91.95, Last Price: 171.30, Change: 79.35, Total Profit: 158.70\n",
      "Stock: RENUKA, First Price: 24.75, Last Price: 31.75, Change: 7.00, Total Profit: 7.00\n",
      "Stock: RCOM, First Price: 72.10, Last Price: 73.90, Change: 1.80, Total Profit: 1.80\n",
      "Stock: TATAMOTORS, First Price: 183.95, Last Price: 312.65, Change: 128.70, Total Profit: 128.70\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Selected stocks, based on the analysis\n",
    "stock_list = [\"ITC\", \"SHREEASHTA\", \"GVKPIL\", \"IDFC\", \"RENUKA\", \"RCOM\", \"TATAMOTORS\"]\n",
    "multiplier = [2, 1, 2, 2, 1, 1, 1]\n",
    "\n",
    "# Step 12: Simulate the investment strategy based on 2012 data (testing period)\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "for the_stock, the_multiplier in zip(stock_list, multiplier):\n",
    "    try:\n",
    "        first_day_price = get_price_on_day(the_stock, df_2012_first_day)\n",
    "        last_day_price = get_price_on_day(the_stock, df_2012_last_day)\n",
    "        diff = (last_day_price - first_day_price)\n",
    "        total_diff = diff * the_multiplier\n",
    "        total_profit += total_diff\n",
    "        total_investment += (first_day_price * the_multiplier)\n",
    "        prices.append([the_stock, first_day_price, last_day_price, diff, total_diff])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# print(prices)\n",
    "\n",
    "# Step 13: Display results\n",
    "print(\"Investment Summary:\")\n",
    "print(f\"Total Investment: {total_investment:.2f}\")\n",
    "print(f\"Total Profit: {total_profit:.2f}\")\n",
    "print(f\"Profit Percentage: {total_profit / total_investment * 100:.2f}%\")\n",
    "\n",
    "# Display prices for each stock\n",
    "for result in prices:\n",
    "    print(f\"Stock: {result[0]}, First Price: {result[1]:.2f}, Last Price: {result[2]:.2f}, Change: {result[3]:.2f}, Total Profit: {result[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|    SYMBOL|   avg(TOTTRDQTY)|\n",
      "+----------+-----------------+\n",
      "|  ALOKTEXT|8677144.275303643|\n",
      "|    GVKPIL|8149384.765182186|\n",
      "|  HINDALCO|7992351.906882592|\n",
      "|      RCOM|7713584.137651822|\n",
      "|    RENUKA|7459392.910931174|\n",
      "|SHREEASHTA|7339390.076923077|\n",
      "|       ITC|7325373.246963562|\n",
      "|      IDFC|7102852.137651822|\n",
      "|      HDIL|6585712.372469636|\n",
      "|TATAMOTORS|6425309.267206478|\n",
      "+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----------+------+\n",
      "|  TIMESTAMP|      S1|Close1|        S2|Close2|\n",
      "+-----------+--------+------+----------+------+\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|TATAMOTORS|246.45|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|SHREEASHTA|   4.0|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|    RENUKA| 38.85|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|      RCOM|  96.7|\n",
      "|02-FEB-2012|ALOKTEXT|  20.2|       ITC|199.05|\n",
      "+-----------+--------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|   Symbol1|   Symbol2|       Corr|\n",
      "+----------+----------+-----------+\n",
      "|    GVKPIL|      IDFC|0.025669474|\n",
      "|    RENUKA|      IDFC|  0.2969912|\n",
      "|      RCOM|    GVKPIL|  0.8123094|\n",
      "|TATAMOTORS|       ITC| 0.25560105|\n",
      "|      IDFC|TATAMOTORS|  0.4580152|\n",
      "+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|   Symbol1|   Symbol2|      Corr|\n",
      "+----------+----------+----------+\n",
      "|  HINDALCO|      RCOM|0.87091833|\n",
      "|      HDIL|      IDFC| 0.8498548|\n",
      "|      RCOM|    GVKPIL| 0.8123094|\n",
      "|    GVKPIL|  HINDALCO| 0.7652074|\n",
      "|       ITC|      IDFC|0.70560426|\n",
      "|      RCOM|  ALOKTEXT|0.69144714|\n",
      "|    RENUKA|  HINDALCO|0.68089527|\n",
      "|    GVKPIL|  ALOKTEXT|0.63710546|\n",
      "|  ALOKTEXT|  HINDALCO| 0.6273346|\n",
      "|    RENUKA|    GVKPIL| 0.6138341|\n",
      "|SHREEASHTA|      RCOM| 0.5673514|\n",
      "|SHREEASHTA|  ALOKTEXT|  0.531341|\n",
      "|    RENUKA|      HDIL|0.52904177|\n",
      "|      HDIL|TATAMOTORS|0.50435644|\n",
      "|      RCOM|    RENUKA|0.49194598|\n",
      "|      IDFC|TATAMOTORS| 0.4580152|\n",
      "|  HINDALCO|SHREEASHTA|0.45538712|\n",
      "|    GVKPIL|      HDIL|0.40701145|\n",
      "|       ITC|      HDIL|0.39094618|\n",
      "|SHREEASHTA|    GVKPIL|0.35677636|\n",
      "+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4450:================================>                     (12 + 8) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day of 2012: 02-JAN-2012, Last day of 2012: 31-DEC-2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Step 1: Filter for 2011 data to train the model\n",
    "df_2011 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2011'\")\n",
    "df_2011_avgqty = df_2011.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "df_2011_avgqty.show(10)\n",
    "\n",
    "# Step 2: Select the top 10 stocks for training\n",
    "top10_2011 = df_2011_avgqty.limit(10)\n",
    "t1 = top10_2011.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Step 3: Filter the data for 2012 testing based on selected stocks\n",
    "df_2012 = df.filter(\"SERIES == 'EQ'\").filter(\"TIMESTAMP like '%2012'\")\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "\n",
    "# Step 4: Prepare for correlation calculation (join data for pairwise correlations)\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "df_for_corr = t3.join(t4, \"TIMESTAMP\")\n",
    "df_for_corr.show(5)\n",
    "\n",
    "# Step 5: Get distinct pairs for correlation calculation\n",
    "wrklist = df_for_corr.select(\"S1\", \"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "\n",
    "# Step 6: Calculate correlation for each pair (only positive correlation)\n",
    "tcorr = []\n",
    "tlen = len(wrklist)\n",
    "for i in range(tlen):\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\", \"Close2\")\n",
    "    if corr > 0:  # Only include pairs with positive correlation\n",
    "        tcorr.append([s1, s2, corr])\n",
    "    # if ((i + 1) % 10 == 0):\n",
    "    #     print(f\"Processed: {i + 1} of {tlen}\", end='')\n",
    "\n",
    "# Step 7: Create a DataFrame for positive correlations\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "rdd = sc.parallelize(tcorr)\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "df_corr.show(5)\n",
    "\n",
    "# Step 8: Filter positive correlations and analyze the portfolio\n",
    "df_corr_pos = df_corr.filter(F.col(\"Corr\") > 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").desc())\n",
    "df_corr_pos.count()\n",
    "\n",
    "df_corr_pos.show()\n",
    "\n",
    "# Step 9: Get first and last days of 2012 for selected stocks (testing period)\n",
    "df_2012_first_day = df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2012'\").distinct().orderBy(\"TIMESTAMP\").first()[0]\n",
    "df_2012_last_day = df_2012.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2012'\").distinct().orderBy(\"TIMESTAMP\", ascending=False).first()[0]\n",
    "\n",
    "print(f\"First day of 2012: {df_2012_first_day}, Last day of 2012: {df_2012_last_day}\")\n",
    "\n",
    "# Step 10: Function to get price on a specific day\n",
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    loc_price = df_2012.where(F.col(\"TIMESTAMP\") == loc_date).where(F.col(\"SYMBOL\") == loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    return (loc_price)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investment Summary:\n",
      "Total Investment: 964.05\n",
      "Total Profit: 594.70\n",
      "Profit Percentage: 61.69%\n",
      "Stock: ITC, First Price: 198.65, Last Price: 286.80, Change: 88.15, Total Profit: 88.15\n",
      "Stock: GVKPIL, First Price: 12.40, Last Price: 13.55, Change: 1.15, Total Profit: 2.30\n",
      "Stock: IDFC, First Price: 91.95, Last Price: 171.30, Change: 79.35, Total Profit: 238.05\n",
      "Stock: RENUKA, First Price: 24.75, Last Price: 31.75, Change: 7.00, Total Profit: 7.00\n",
      "Stock: RCOM, First Price: 72.10, Last Price: 73.90, Change: 1.80, Total Profit: 1.80\n",
      "Stock: TATAMOTORS, First Price: 183.95, Last Price: 312.65, Change: 128.70, Total Profit: 257.40\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Selected stocks based on the positive correlation analysis\n",
    "# These are the stocks selected based on their positive correlations\n",
    "stock_list = [\"ITC\", \"GVKPIL\", \"IDFC\", \"RENUKA\", \"RCOM\", \"TATAMOTORS\"]\n",
    "multiplier = [1, 2, 3, 1, 1, 2]\n",
    "\n",
    "# Step 12: Simulate the investment strategy based on 2012 data (testing period)\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "for the_stock, the_multiplier in zip(stock_list, multiplier):\n",
    "    first_day_price = get_price_on_day(the_stock, df_2012_first_day)\n",
    "    last_day_price = get_price_on_day(the_stock, df_2012_last_day)\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    prices.append([the_stock, first_day_price, last_day_price, diff, total_diff])\n",
    "\n",
    "# Step 13: Display results\n",
    "print(\"Investment Summary:\")\n",
    "print(f\"Total Investment: {total_investment:.2f}\")\n",
    "print(f\"Total Profit: {total_profit:.2f}\")\n",
    "print(f\"Profit Percentage: {total_profit / total_investment * 100:.2f}%\")\n",
    "\n",
    "# Display prices for each stock\n",
    "for result in prices:\n",
    "    print(f\"Stock: {result[0]}, First Price: {result[1]:.2f}, Last Price: {result[2]:.2f}, Change: {result[3]:.2f}, Total Profit: {result[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We find that the highest profit percentage is found when we take the stocks which are positively correlated, and invest in them. This is because when the stocks are positively correlated, they tend to move in the same direction, and hence the risk is higher, but the reward is also higher. This is reflected in the higher profit percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
