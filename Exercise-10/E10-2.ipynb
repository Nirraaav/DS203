{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E10-2\n",
    "#### This Notebook illustrates the use of SPARK Dataframe functions to process nsedata.csv\n",
    "- Review <b>Part-1</b> to understand the code by referring to SPARK documentation.\n",
    "- Add your comment to each cell, to explain its purpose\n",
    "- Add code / create additional cells for debugging purpose, and comment them too \n",
    "- Write SPARK code to solve the problem stated in <b>Part-2</b> (do not use the createTempView function in your solution!)\n",
    "\n",
    "<b>Submission</b>\n",
    "- Create and upload a PDF of this Notebook.\n",
    "- <b> BEFORE CONVERTING TO PDF ENSURE THAT YOU REMOVE / TRIM LENGTHY DEBUG OUTPUTS </b>.\n",
    "- Short debug outputs of up to 5 lines are acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 04:00:13 WARN Utils: Your hostname, maverick resolves to a loopback address: 127.0.1.1; using 192.168.206.245 instead (on interface wlp0s20f3)\n",
      "24/11/07 04:00:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 04:00:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"E9-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/07 04:00:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "dfr = ss.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('SYMBOL', StringType(), True), StructField('SERIES', StringType(), True), StructField('OPEN', DoubleType(), True), StructField('HIGH', DoubleType(), True), StructField('LOW', DoubleType(), True), StructField('CLOSE', DoubleType(), True), StructField('LAST', DoubleType(), True), StructField('PREVCLOSE', DoubleType(), True), StructField('TOTTRDQTY', LongType(), True), StructField('TOTTRDVAL', DoubleType(), True), StructField('TIMESTAMP', StringType(), True), StructField('ADDNL', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaStruct = StructType()\n",
    "schemaStruct.add(\"SYMBOL\", StringType(), True)\n",
    "schemaStruct.add(\"SERIES\", StringType(), True)\n",
    "schemaStruct.add(\"OPEN\", DoubleType(), True)\n",
    "schemaStruct.add(\"HIGH\", DoubleType(), True)\n",
    "schemaStruct.add(\"LOW\", DoubleType(), True)\n",
    "schemaStruct.add(\"CLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"LAST\", DoubleType(), True)\n",
    "schemaStruct.add(\"PREVCLOSE\", DoubleType(), True)\n",
    "schemaStruct.add(\"TOTTRDQTY\", LongType(), True)\n",
    "schemaStruct.add(\"TOTTRDVAL\", DoubleType(), True)\n",
    "schemaStruct.add(\"TIMESTAMP\", StringType(), True)\n",
    "schemaStruct.add(\"ADDNL\", StringType(), True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfr.csv(\"/home/hduser/spark/nsedata.csv\", schema=schemaStruct, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Basics : Using SPARK for analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_from_df(company_code):\n",
    "    tcode = company_code.lower()\n",
    "    df_subset = df.select(\\\n",
    "                    F.col(\"OPEN\").alias(\"OPEN_\" + tcode),\\\n",
    "                    F.col(\"HIGH\").alias(\"HIGH_\"+ tcode),\\\n",
    "                    F.col(\"LOW\").alias(\"LOW_\"+ tcode),\\\n",
    "                    F.col(\"CLOSE\").alias(\"CLOSE_\" + tcode),\\\n",
    "                    F.col(\"TIMESTAMP\")).\\\n",
    "                    where(F.col(\"SYMBOL\") == company_code)\n",
    "    return(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need to use the alias function, above? What happens if we do not alias / rename the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infy = create_subset_from_df(\"INFY\")\n",
    "df_infy.show(5)\n",
    "df_infy.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tcs = create_subset_from_df(\"TCS\")\n",
    "df_tcs.show(5)\n",
    "df_tcs.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_tcs.join(df_infy,\"TIMESTAMP\").select(\"TIMESTAMP\",\"CLOSE_tcs\",\"CLOSE_infy\")\n",
    "df_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.select(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]).alias(\"PriceDiff\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.filter(F.abs(df_join[\"CLOSE_tcs\"] - df_join[\"CLOSE_infy\"]) < 180).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "df1 = df.withColumn(\"TIMESTAMP2\", date_format(to_date(col(\"TIMESTAMP\"), \"dd-MMM-yyyy\"), \"yyyy-MM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_t1 = df1.groupBy(\"SYMBOL\",\"TIMESTAMP2\").agg(F.min(\"OPEN\"), F.max(\"OPEN\"), F.avg(\"OPEN\"),\\\n",
    "                                       F.stddev(\"OPEN\"), F.count(\"OPEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2 = df_t1.sort(F.asc(\"SYMBOL\"), F.asc(\"TIMESTAMP2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following statement to generate the output, and analyze it\n",
    "# Write your observations in the next cell\n",
    "\n",
    "#df_t2.write.csv(\"monthly_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observation related to monthly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <b>SPARK based solutions for stock analysis and portfolio management: An Example</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Based on equity (EQ) data contained in nsedata.csv, you are tasked with the responsibility to identify a set of 10 stocks to invest in based on the following steps:\n",
    "\n",
    "- You have to process the data for one entire year, and then make investment decisions for the following year. You can shoose 2012 as the past year and make recommendations for 2013.\n",
    "- Assume that you are doing this analysis on Jan 1, 2013. \n",
    "- You are required to draw up an initial list of 10 stocks based on the following preliminary analysis: \n",
    "    - The stocks should be liquid. That is, they should be traded in large volumes almost every day and the trading volume should be high.\n",
    "    - You have to filter those stocks that have shown maximum overall growth over the past year. The hope is that they will continue to grow in the future.\n",
    "- Select 5 pairs of stocks from these filtered stocks based on the following further analysis.\n",
    "    - You should ensure that volatility and negative market movements in the coming year will not adversely affect the total investment, substantially.\n",
    "    - One way to achieve this involves selecting <b>stock pairs that are negatively correlated</b>, so that if one stock loses value its partner will most likely gain value - thereby reducing the overall impact of fall in stock prices. As all these stocks are high growth stocks, anyway, the expectation is that there also will be overall growth of the portfolio. \n",
    "    - Purchase 1 unit of each of these stock pairs on the first trading day of the next year (i.e. 2013)\n",
    "- Once you have selected the 5 pairs and made the above investments, you should further do the following\n",
    "    - Report the performance of your portfolio as on 31/12/2013 (or the nearest traded date, if 31/12/2013 was a non traded day) in terms of the:\n",
    "        - Overall growth of your portfolio\n",
    "        - Report which stocks in your portfolio grew in value, which of them reduced in value, an whether the pairing strategy worked.\n",
    "        - How did the overall market perform during the same period? This can be assessed as follows:\n",
    "            - If you had blindly selected 1 stock each of the top 25 highly traded, high growth stocks, what would have been the performance of this portfolio\n",
    "            - How did the implemented strategy of selecting highly traded, high growth stocks, but in pairs having <b>negative correlation</b>, perform in comparion? Did the strategy work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some suggested steps to solve the problem\n",
    "\n",
    "# First of all select only EQUITY related data\n",
    "# Create a dataframe of stocks that have traded in during the year 2012\n",
    "# Find out the average total traded quantity of each of these stocks\n",
    "# Identify stocks that high trade volumes: average daily volume ranging between 5L and 10L\n",
    "# Find out the price difference in each of these stocks between the 'last traded day of 2012' and 'first traded day of 2012'\n",
    "# Sort the stocks in descending order using traded quantity and price difference as the criteria\n",
    "# Select the top 10 stocks for further analysis\n",
    "# Create a new dataframe containing pairs of stocks traded on the same day \n",
    "#   - join the selected stocks by using the criteria that stock names in the resulting dataframe are different\n",
    "# Sort the dataframe in ascending order\n",
    "# Establish the criteria for selecting the final pairs of stocks, and select them\n",
    "# Calculate your total investment value\n",
    "# ... likewise state and complete the rest of the steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2012'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012_avgqty = df_2012.groupBy(\"SYMBOL\").avg(\"TOTTRDQTY\")\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") < 10000000)\\\n",
    "                        .filter(F.col(\"avg(TOTTRDQTY)\") > 500000)\\\n",
    "                        .orderBy(\"avg(TOTTRDQTY)\", ascending=False)\n",
    "df_2012_avgqty.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df_2012_avgqty.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = top10.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "t2 = df_2012.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "t3 = t2.select(F.col(\"SYMBOL\").alias(\"S1\"), F.col(\"CLOSE\").alias(\"Close1\"), \"TIMESTAMP\")\n",
    "t4 = t2.select(F.col(\"SYMBOL\").alias(\"S2\"), F.col(\"CLOSE\").alias(\"Close2\"), \"TIMESTAMP\")\n",
    "df_for_corr = t3.join(t4,\"TIMESTAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrklist = df_for_corr.select(\"S1\",\"S2\").filter(\"S1 != S2\").distinct().collect()\n",
    "wrklist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wrklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL TAKES QUITE SOME TIME TO EXECUTE - BE PATIENT!\n",
    "tcorr = []\n",
    "tlen = len(wrklist)\n",
    "for i in range(tlen):\n",
    "    s1 = wrklist[i][0]\n",
    "    s2 = wrklist[i][1]\n",
    "    corr = df_for_corr.filter((F.col('S1') == s1) & (F.col('S2') == s2)).corr(\"Close1\",\"Close2\")\n",
    "    tcorr.append([s1,s2,corr])\n",
    "    if(i%20 ==0):\n",
    "        print(f\"Processed: {i} of {tlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol1\", StringType(), True),\n",
    "    StructField(\"Symbol2\", StringType(), True),\n",
    "    StructField(\"Corr\", FloatType(), True)\n",
    "])\n",
    "\n",
    "rdd = sc.parallelize(tcorr)\n",
    "df_corr = ss.createDataFrame(rdd.map(lambda x: Row(Symbol1=x[0], Symbol2=x[1], Corr=float(x[2]))), schema)\n",
    "\n",
    "df_corr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_neg = df_corr.filter(F.col(\"Corr\") <= 0.0).dropDuplicates([\"Corr\"]).orderBy(F.col(\"Corr\").asc())\n",
    "df_corr_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_neg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = df.filter(\"SERIES=='EQ'\").filter(\"TIMESTAMP like '%2013'\")\n",
    "\n",
    "first_day_2013 = (df_2013.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%JAN-2013'\").distinct().orderBy(\"TIMESTAMP\").first())[0]\n",
    "last_day_2013 = (df_2013.select(\"TIMESTAMP\").filter(\"TIMESTAMP like '%DEC-2013'\").distinct().orderBy(\"TIMESTAMP\",ascending=False).first())[0]\n",
    "\n",
    "print(first_day_2013,last_day_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_on_day(loc_stock, loc_date):\n",
    "    loc_price = df_2013.where(F.col(\"TIMESTAMP\")==loc_date).where(F.col(\"SYMBOL\")==loc_stock).select(\"CLOSE\").collect()[0]\n",
    "    return((loc_price)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected stocks, based on the analysis\n",
    "# |     ITC|ALOKTEXT|  -0.90314275|\n",
    "# |GMRINFRA|     ITC|   -0.7135044|\n",
    "# |    IDFC|ALOKTEXT|   -0.6409445|\n",
    "# |HINDALCO|     ITC|  -0.62534785|\n",
    "# |ALOKTEXT|    NHPC|  -0.33097458|\n",
    "\n",
    "stock_list = [\"ITC\",\"ALOKTEXT\",\"GMRINFRA\",\"IDFC\",\"HINDALCO\",\"NHPC\"]\n",
    "multiplier = [3,3,1,1,1,1]\n",
    "\n",
    "prices = []\n",
    "total_profit = 0\n",
    "total_investment = 0\n",
    "for the_stock,the_multiplier in zip(stock_list,multiplier):\n",
    "    first_day_price = get_price_on_day(the_stock,first_day_2013)\n",
    "    last_day_price  = get_price_on_day(the_stock,last_day_2013)\n",
    "    diff = (last_day_price - first_day_price)\n",
    "    total_diff = diff * the_multiplier\n",
    "    total_profit += total_diff\n",
    "    total_investment += (first_day_price * the_multiplier)\n",
    "    prices.append([the_stock,first_day_price,last_day_price,diff,total_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_investment, total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25 = df_2012_avgqty.limit(25)\n",
    "t1 = top25.select(\"SYMBOL\").rdd.flatMap(lambda x: x).collect()\n",
    "t2 = df_2013.filter(F.col(\"SYMBOL\").isin(t1))\n",
    "t2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day_overall = t2.where(F.col(\"TIMESTAMP\")==first_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "last_day_overall = t2.where(F.col(\"TIMESTAMP\")==last_day_2013).select(\"CLOSE\").agg(F.sum(\"CLOSE\")).collect()\n",
    "total_profit_overall = last_day_overall[0][0] - first_day_overall[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount: {first_day_overall[0][0]} invested on the first trading day of 2013\\n\\\n",
    "has a value: {last_day_overall[0][0]} on the last trading day of 2013\\n\\\n",
    "The profit/loss is : {total_profit_overall:.2f} corresponding to {total_profit_overall/first_day_overall[0][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>Performance of the strategy</b>\n",
    "- If we had invested in all the top 25 stocks, without implementing the negative correlation strategy, \n",
    "There would have been a loss of 892 on an investment of 5119 (17.5% loss)\n",
    "- As against that, by implementing the 'select based on negative correlation' strategy, \n",
    "a profit of 18.2 on an investment of 1249 (1.5% profit) has been achieved\n",
    "- In conclusion, the strategy has definitely prevented portfolio value loss during a bad year. It has, in fact, preserved capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Part 2 : <b>Problem to solve</b></b>\n",
    "1. Which of the following is better, if you have 10 Lakhs to invest for a year: \n",
    "    - identify 5 top performing stocks of the previous year and invest in them, or\n",
    "    - Spread your investment across a basket of 25 stocks, with investments equally distributed among them\n",
    "    - Employing strategies like 'negative correlation' to select your stocks\n",
    "    - What if you use 'positive correlation' instead, carry out analysis to understand the portfolio's performance?\n",
    "2. Do your analysis over multiple years (2011-1012, 2012-2013, etc.) to make your final recommendations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
